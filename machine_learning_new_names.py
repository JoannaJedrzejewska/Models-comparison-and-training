# -*- coding: utf-8 -*-
"""machine learning new names.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bVPsN_m810UtLCrE3VdfjCa3L8FznwuT
"""

###Training machine learning models based on comparison of new names in your databases
import os
import pandas as pd
import numpy as np
import re
import pickle
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import OneClassSVM, SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.model_selection import train_test_split, GridSearchCV
import xgboost as xgb
from bayes_opt import BayesianOptimization
from textblob import TextBlob
from sklearn.preprocessing import StandardScaler
import json

#Define paths
base_path = "C:\\your\\base\\path"
file_pattern = re.compile(r"^file_pattern.*\.xlsx$")

#List folders
folders = [os.path.join(base_path, f) for f in os.listdir(base_path) if re.match(r"^end of the folder name.*", f)]
if not folders:
    raise FileNotFoundError("No folder - check name")

folders_info = pd.DataFrame({"folder": folders, "creation_time": [os.path.getctime(f) for f in folders]})
folders_info = folders_info.sort_values(by="creation_time", ascending=False)

#Select latest and previous folder
newest_folder = folders_info.iloc[0]['folder']
previous_folder = folders_info.iloc[1]['folder']

print("Newest folder:", newest_folder)
print("Previous folder:", previous_folder)

#Select files
newest_file = [f for f in os.listdir(newest_folder) if file_pattern.match(f)]
previous_file = [f for f in os.listdir(previous_folder) if file_pattern.match(f)]

if not newest_file or not previous_file:
    raise FileNotFoundError("wrong file - check the name")

new_data = pd.read_excel(os.path.join(newest_folder, newest_file[0]), skiprows=2)
prev_data = pd.read_excel(os.path.join(previous_folder, previous_file[0]), skiprows=2)

#Compare column names
new_col = set(new_data.columns) - set(prev_data.columns)
removed_col = set(prev_data.columns) - set(new_data.columns)
print("New columns:", new_col)
print("Deleted columns:", removed_col)

#Compare rows
new_rows = new_data.loc[~new_data.apply(lambda x: ' '.join(map(str, x)), axis=1).isin(prev_data.apply(lambda x: ' '.join(map(str, x)), axis=1))]
removed_rows = prev_data.loc[~prev_data.apply(lambda x: ' '.join(map(str, x)), axis=1).isin(new_data.apply(lambda x: ' '.join(map(str, x)), axis=1))]

print("Amount of new rows:", len(new_rows))
print("Amount of deleted rows:", len(removed_rows))

#Text cleaning
def clean_text(text):
    text = text.upper()
    text = re.sub(r"[\W_]+", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

#Feature engineering
new_data['Clean_Name'] = new_data['name'].apply(clean_text)
prev_data['Clean_Name'] = prev_data['name'].apply(clean_text)

#TF-IDF Vectorization
vectorizer = TfidfVectorizer()
X_new = vectorizer.fit_transform(new_data['Clean_Name'])
X_prev = vectorizer.transform(prev_data['Clean_Name'])

#Anomaly detection with One-Class SVM
ocsvm = OneClassSVM(nu=0.05, kernel='rbf')
ocsvm.fit(X_prev.toarray())
anomaly_scores = ocsvm.decision_function(X_new.toarray())
new_data['Anomaly_Score'] = anomaly_scores

#Machine Learning Models
X = new_data[['Anomaly_Score']]
y = np.random.choice([0, 1], size=len(new_data))  #Placeholder target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Random Forest
rf_model = RandomForestClassifier(n_estimators=500)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))

#Logistic Regression
log_model = LogisticRegression()
log_model.fit(X_train, y_train)
y_pred_log = log_model.predict(X_test)
print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_log))

#XGBoost
train_matrix = xgb.DMatrix(X_train, label=y_train)
test_matrix = xgb.DMatrix(X_test, label=y_test)
xgb_model = xgb.train({'max_depth': 6, 'eta': 0.3, 'objective': 'binary:logistic'}, train_matrix, num_boost_round=100)
y_pred_xgb = (xgb_model.predict(test_matrix) > 0.5).astype(int)
print("XGBoost Accuracy:", accuracy_score(y_test, y_pred_xgb))

#Bayesian Optimization for SVM
def svm_bayes_opt(cost, gamma):
    model = SVC(kernel='rbf', C=cost, gamma=gamma)
    model.fit(X_train, y_train)
    return accuracy_score(y_test, model.predict(X_test))

optimizer = BayesianOptimization(f=svm_bayes_opt, pbounds={'cost': (0.01, 10), 'gamma': (0.001, 1)}, random_state=42)
optimizer.maximize(init_points=5, n_iter=10)

best_params = optimizer.max['params']
best_svm_model = SVC(kernel='rbf', C=best_params['cost'], gamma=best_params['gamma'])
best_svm_model.fit(X_train, y_train)
print("Best SVM Accuracy (Bayesian Optimization):", accuracy_score(y_test, best_svm_model.predict(X_test)))

#Saving models
save_directory = "C:\\your\\saving\\directory\\for\\models"
os.makedirs(save_directory, exist_ok=True)

with open(os.path.join(save_directory, "rf_model.pkl"), "wb") as f:
    pickle.dump(rf_model, f)
with open(os.path.join(save_directory, "log_model.pkl"), "wb") as f:
    pickle.dump(log_model, f)
xgb_model.save_model(os.path.join(save_directory, "xgb_model.json"))
with open(os.path.join(save_directory, "best_svm_model.pkl"), "wb") as f:
    pickle.dump(best_svm_model, f)

print("Models saved successfully.")
###Choose the model which is the best for your data and load it :)